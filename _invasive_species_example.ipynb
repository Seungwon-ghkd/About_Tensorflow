{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Dataset API를 활용하여 Kaggle 데이터 학습 시켜보기\n",
    "## 예제 : [Invasive Species Monitoring](https://www.kaggle.com/c/invasive-species-monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# pillow\n",
    "from PIL import Image\n",
    "# opencv-python\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\dl_study\\00_project\n"
     ]
    }
   ],
   "source": [
    "cd 00_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. pandas 라이브러리를 통해 라벨 정보가 들어있는 csv 파일을 읽어서 file_list에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  invasive\n",
       "0     1         0\n",
       "1     2         0\n",
       "2     3         1\n",
       "3     4         0\n",
       "4     5         1\n",
       "5     6         0\n",
       "6     7         1\n",
       "7     8         1\n",
       "8     9         0\n",
       "9    10         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = \"./Invasive_Species/dataset/\"\n",
    "\n",
    "# pandas로 csv 파일 읽어오기\n",
    "file_list = pd.read_csv(train_dir + 'train_labels.csv')\n",
    "# pandas DataFrame의 data type 확인\n",
    "print(type(file_list))\n",
    "# csv 파일을 잘 읽어왔나 상위 10개의 데이터 확인\n",
    "file_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas로 파일 이름과 라벨 정보를 읽어오는 테스트로  \n",
    "5번째 파일의 이름과 ----> file_list.iloc[4][0]  \n",
    "5번째 파일의 라벨을 ----> file_list.iloc[4][1]  \n",
    "시험삼아 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# DataFrame.head() 로 출력한 결과와 일치하는가 확인\n",
    "print(file_list.iloc[4][0]) # [5번째 줄][name] 출력\n",
    "print(file_list.iloc[4][1]) # [5번째 줄][invasive] 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset 구성 밑준비\n",
    "x_images에는 jpg 파일의 이름과 경로를 저장 (dtype=string)  \n",
    "y_labels에는 class(0 또는 1)를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뽑아올 샘플 수 설정 (임의로 100개만 사용)\n",
    "n_samples = 100\n",
    "# 빈 list형 변수 만들어두기\n",
    "x_images, y_labels = [], []\n",
    "# 샘플 수 만큼 for문 반복\n",
    "for i in range(n_samples):\n",
    "    # 파일이 저장된 경로와 이름, 확장자를 문자형(string)으로 변수 x_images에 차곡차곡 저장(append)\n",
    "    x_images.append(train_dir + 'train/' + str(file_list.iloc[i][0]) + '.jpg')\n",
    "    # y_labels에 invasive 정보 저장\n",
    "    if file_list.iloc[i][1] == 0:\n",
    "        # invasive가 0이면 y_labels에 0으로 저장\n",
    "        y_labels.append(0)\n",
    "    else:\n",
    "        # invasive가 0이 아니면 y_labels에 1로 저장\n",
    "        y_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Invasive_Species/dataset/train/1.jpg',\n",
       " './Invasive_Species/dataset/train/2.jpg',\n",
       " './Invasive_Species/dataset/train/3.jpg',\n",
       " './Invasive_Species/dataset/train/4.jpg',\n",
       " './Invasive_Species/dataset/train/5.jpg',\n",
       " './Invasive_Species/dataset/train/6.jpg',\n",
       " './Invasive_Species/dataset/train/7.jpg',\n",
       " './Invasive_Species/dataset/train/8.jpg',\n",
       " './Invasive_Species/dataset/train/9.jpg',\n",
       " './Invasive_Species/dataset/train/10.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#잘 되었나 10개만 시험삼아 출력\n",
    "x_images[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 0, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset 구성  \n",
    "2에서 만든 파일 목록과 라벨 목록을 tensor로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_images = tf.constant(x_images)\n",
    "tf_labels = tf.constant(y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor로 만든 파일 목록과 라벨 목록으로 Dataset을 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((tf_images, tf_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구성된 Dataset을 통해  \n",
    "- 파일 목록에서 파일을 읽어오고 (tf.read_file)  \n",
    "- 읽어온 이미지 파일을 decode (tf.image.decode_jpeg)  \n",
    "- decode된 이미지를 resize (tf.image.resize_images)  \n",
    "- 라벨 목록은 one hot encoding (tf.one_hot)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_images(image, label):\n",
    "    x = tf.read_file(image) # file read \n",
    "    x = tf.image.decode_jpeg(x, channels=3) # 컬러 이미지라면 channels=3, 흑백이라면 channels=1\n",
    "    x = tf.image.resize_images(x, [100, 100]) # 이미지 크기를 100 x 100으로 resize\n",
    "\n",
    "    y = tf.one_hot(label, depth=2) # 라벨을 one hot vector로 만들어준다. class는 2개\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataset 설정\n",
    "- 학습에 사용될 Dataset을 위에서 구성한 decode_image function을 넣어주고(map)\n",
    "- mini batch를 구성해준다. (batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(decode_images).batch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset을 iterable하게 해주기 위해 Iterator를 만들고  \n",
    "Iterator가 실행될 때마다 반환되는 값을 설정(get_next)  \n",
    "iterator initializer에 사용할 dataset 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator 만들기\n",
    "data_iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n",
    "x_image, y_label = data_iterator.get_next()\n",
    "\n",
    "# iterator 초기화를 위해 initializer 생성\n",
    "init_iter = data_iterator.make_initializer(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.float32)\n",
      "(TensorShape([Dimension(None), Dimension(100), Dimension(100), Dimension(3)]), TensorShape([Dimension(None), Dimension(2)]))\n"
     ]
    }
   ],
   "source": [
    "# iterator에 type과 shape 확인\n",
    "print(train_data.output_types)\n",
    "print(train_data.output_shapes)\n",
    "# tensorflow의 자료형은 아래와 같은 형태가 일반적임\n",
    "# 이미지의 경우 : [batch][height][width][channel]\n",
    "# 라벨의 경우  : [batch][class]\n",
    "# 위와 같이 잘 나오나 확인\n",
    "\n",
    "# batch은 기본적으로 None으로 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = False\n",
    "\n",
    "# build model to predict Y\n",
    "conv = tf.layers.conv2d(x_image, 32, 3, 1, 'same', activation=tf.nn.relu)\n",
    "conv = tf.layers.conv2d(x_image, 32, 3, 1, 'same', activation=tf.nn.relu)\n",
    "conv = tf.layers.dropout(conv, 0.25, training=is_training)\n",
    "conv = tf.layers.max_pooling2d(conv, 5, 5)\n",
    "conv = tf.layers.conv2d(conv, 64, 3, 1, 'same', activation=tf.nn.relu)\n",
    "conv = tf.layers.conv2d(conv, 64, 3, 1, 'same', activation=tf.nn.relu)\n",
    "conv = tf.layers.dropout(conv, 0.25, training=is_training)\n",
    "conv = tf.layers.max_pooling2d(conv, 5, 5)\n",
    "conv = tf.layers.conv2d(conv, 128, 3, 1, 'same', activation=tf.nn.relu)\n",
    "conv = tf.layers.conv2d(conv, 128, 3, 1, 'same', activation=tf.nn.relu)\n",
    "conv = tf.layers.dropout(conv, 0.25, training=is_training)\n",
    "conv = tf.layers.average_pooling2d(conv, 4, 1)\n",
    "conv = tf.layers.flatten(conv)\n",
    "logits = tf.layers.dense(conv, 2, name='Logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define loss function / optimizer / prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Loss'):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_label, logits=logits)\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.name_scope('Prediction'):\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(y_label, 1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 학습 시작 및 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 95.5413\n",
      "Accuracy 0.6600\n",
      "Epoch 2: loss = 0.0664\n",
      "Accuracy 0.6600\n",
      "Epoch 3: loss = 0.0654\n",
      "Accuracy 0.6600\n",
      "Epoch 4: loss = 0.0670\n",
      "Accuracy 0.6600\n",
      "Epoch 5: loss = 0.0622\n",
      "Accuracy 0.6600\n",
      "Epoch 6: loss = 0.0604\n",
      "Accuracy 0.6600\n",
      "Epoch 7: loss = 0.0571\n",
      "Accuracy 0.6700\n",
      "Epoch 8: loss = 0.0519\n",
      "Accuracy 0.7900\n",
      "Epoch 9: loss = 0.0448\n",
      "Accuracy 0.8800\n",
      "Epoch 10: loss = 0.0370\n",
      "Accuracy 0.8300\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 10\n",
    "step = 0\n",
    "\n",
    "# session 시작\n",
    "with tf.Session() as sess:  \n",
    "    # 모든 변수(variables)를 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Tensorboard에 그래프 저장\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "\n",
    "    for i in range(total_epochs): \n",
    "        # 학습을 위한 Session 시작\n",
    "        sess.run(init_iter) # Dataset API로 data를 넣어주기 위해 train data용 iterator 초기화 실행\n",
    "        is_training = True\n",
    "        total_loss = 0\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                _, l = sess.run([optimizer, loss])\n",
    "                step += 1\n",
    "                total_loss += l\n",
    "\n",
    "        # train_data를 모두 사용해서 계산했다면 (MNIST의 train_data의 경우 55,000번)\n",
    "        # tf.errors.OutOfRangeError가 발생하는데\n",
    "        # 이 에러가 발생했다면 1번의 epoch이 끝났다는 것을 의미하기 때문에 아무 것도 안하고 그냥 넘어감(pass)\n",
    "        except tf.errors.OutOfRangeError: \n",
    "            pass\n",
    "        \n",
    "        print('Epoch {0}: loss = {1:0.4f}'.format(i+1, total_loss/n_samples))\n",
    "        \n",
    "        # Test Session\n",
    "        is_training = False\n",
    "        sess.run(init_iter)\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch = sess.run([accuracy])\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Accuracy {0:0.4f}'.format(total_correct_preds/n_samples))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input data가 적어서 accuracy는 충분치 않지만 loss가 줄어드는 것을 보아 잘 학습되고 있는 것으로 생각할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
